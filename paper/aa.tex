%-----------------------------------------------------------------------
%                                                                 aa.tex
% AA vers. 9.3, LaTeX class for Astronomy & Astrophysics
%-----------------------------------------------------------------------
\documentclass{aa}  
\usepackage{graphicx}
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{indentfirst}

% ==== 超链接包 ====
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}
\title{Predicting NBA Team Performance: \\
From Historical Standings to Player-level Forecasts}

\author{
    Weihao Li\inst{1}
    % \fnmsep\thanks{Corresponding author: wli@u.northwestern.edu}
    \and Shuhao Gao\inst{1}
    \and Shijie Chen\inst{1}
    \and Anbang Liu\inst{1}
}

\institute{
    McCormick School of Engineering, Northwestern University \\
    \email{\{WeihaoLi2027, ShuhaoGao2027, ShijieChen2027, AnbangLiu2027\}@u.northwestern.edu}
}

% \date{Received December 8, 2025}

%-----------------------------------------------------------------
% ABSTRACT
%-----------------------------------------------------------------
\abstract
{Publicly available NBA box-score and standings data make it possible to build detailed models of team and player performance, yet many forecasting systems rely on simple team-level summaries such as past wins or win percentage. In this project, we systematically compare team-centric and player-centric pipelines for predicting regular-season team performance and related player outcomes over roughly two decades of NBA data. First, we build a historical-standings baseline that regresses team win percentage on features constructed from past seasons (previous win rate, rolling 3-year and 5-year averages, and short-term trends). Second, we train player-level models on season-long box-score statistics to: (i) predict All-Star selection, (ii) predict whether a player receives All-NBA/MVP--related honors, and (iii) classify players into broad positional groups (Guard / Wing / Big). Next, we roll these player predictions back up to the team level by aggregating player statistics into player-informed team features. These, combined with team box-score summaries, allow us to forecast each season’s win percentage and overall league ranking.
Finally, we implement an automatic model search over feature groups, preprocessing schemes, and regression algorithms, selecting configurations that maximize ranking-based metrics on the two most recent seasons. 
Across held-out years, we find that simple models based on historical win trends achieve strong performance, with high $R^2$ and tight correlation with true win percentage. However, incorporating player-level information and running an automated pipeline search improves ranking accuracy, especially in recent seasons and for teams undergoing major roster changes. Our results suggest that while team-level trends serve as strong baselines, player-level modeling and automated pipeline search add value for capturing league-wide standings and connecting roster composition to expected team performance.}
\keywords{basketball --
          sports analytics --
          machine learning --
          time series forecasting --
          player development}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In today’s NBA, decisions about roster construction, trades, and playing time rely heavily on data. Teams, media, and fans all care about the same question: given everything we know from past seasons, how well can we predict regular-season performance and key player outcomes in a way that’s both accurate and easy to understand? With nearly twenty years of publicly available box-score data and standings, we can now move beyond simple narratives and test which modeling choices actually help and which ones are unnecessary.

A natural first step is to look at each team as a single unit and summarize their history using wins, losses, and a few simple trend features. Similar to many real-world forecasting systems,this approach uses past win percentage and simple trends to project future success. Such models are easy to interpret and compute, but they ignore who is actually on the roster and how individual players evolve in their roles over time. They also struggle with structural changes in a team like a major trade or losing a star to injury.

A player-focused perspective fills in those gaps. Season-level box-score data tracks scoring, playmaking, defensive impact, and usage patterns that include both individual contributions and team results. In this project, we use player data to tackle three prediction problems: (1) identifying All-Star–caliber seasons, (2) predicting whether a player will receive All-NBA or MVP–related honors, and (3) classifying players into broad positional groups (Guard / Wing / Big). These tasks test whether basic box-score data is sufficient to provide meaningful information for how the league evaluates and uses players.

We then link the player and team perspectives by aggregating player statistics back to the team level and combining them with historical win–trend features. On top of this team-level table, we build and evaluate a variety of regression pipelines to predict team win percentage and resulting standings. Rather than handpicking a single model, we run an automatic search over feature groups, preprocessing schemes, and algorithms and select configurations that perform best on ranking-based metrics for the final two seasons.

Together, these components allow us to ask the question: when is a simple team-level win–trend baseline “good enough,” and when are player-level features and automated models more effective in predicting how the NBA standings? This project aims to quantify that trade-off and clarify how much value is added by moving from direct team summaries to more detailed, player-informed representations in NBA data analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prior Literature}

\subsection{Team-level rating and expectation models}

A large body of work in sports analytics has focused on predicting team performance using aggregate team-level statistics. One influential family of models is the \emph{Pythagorean expectation}, which estimates a team's theoretical winning percentage from points scored and points allowed via a power-law relationship. Originally proposed for baseball and later adapted to basketball, this idea underlies many simple baselines that relate scoring margins to wins and losses \citep[see, e.g.,][]{oliver2004basketball,sarlis2020sports}. These models provide strong, easy-to-interpret benchmarks but depend solely on aggregate scoring margins and do not incorporate information about individual players.

Another common approach is to model team strength with rating systems inspired by Elo. In basketball applications, Elo-style ratings are updated after each game based on the result, home-court advantage, and the margin of victory, and then used to forecast future outcomes. Public-facing systems such as FiveThirtyEight's NBA model illustrate how dynamic ratings can track changes in team strength over a season and provide reasonably accurate probabilistic predictions for both games and playoff series \citep{silver2015elo}. Together, Pythagorean and Elo-style systems represent static or quasi-static team-level baselines that are closely related to our first family of models, which relies on historical team wins and win-percentage trends. However, because these methods operate on aggregate team outcomes, they are limited in their ability to anticipate abrupt changes in performance driven by roster turnover or shifts in player roles, and they rarely make explicit use of the rich player-level data that are now widely available.

\subsection{Machine learning for NBA game and season prediction}

Beyond analytic formulas and rating systems, many studies have applied machine learning to predict basketball results using team-level features. Early work by \citet{loeffelholz2009nba} used neural networks to predict single-game NBA outcomes from box-score statistics and contextual variables, demonstrating that nonlinear models can capture interactions between basic team statistics. More recent surveys review a wide range of approaches, including logistic regression, support vector machines, tree-based ensembles, and deep neural networks, and typically find that machine-learning models outperform simpler statistical baselines when sufficient historical data are available \citep{sarlis2020sports}.

Researchers have also moved from game-level prediction to season-level tasks such as forecasting a team's final win total or playoff qualification. For example, \citet{yang2015nba} regress regular-season wins on team-level and aggregated player statistics to study which factors are most predictive of team success. These season-level models again treat each team as the unit of analysis and usually rely on summary statistics from the current or previous season, with a small number of hand-chosen algorithms and loss functions.

In contrast, our project situates season-level win-percentage prediction in a broader model-selection and evaluation framework. We combine simple historical win-percentage trends with richer team- and player-derived features, and systematically compare a large family of models (linear methods, tree ensembles, neural networks, kernel methods, $k$-NN, and simple ensembles) under a common ranking-based evaluation on held-out seasons. This lets us assess not only whether machine learning improves over simple standings-based baselines, but also which combinations of features, preprocessing, and algorithms are most effective for predicting the structure of the final league table.

\subsection{Player-level prediction and its link to team performance}

Complementary to team-level approaches, a growing literature studies player evaluation and performance prediction using detailed box-score and tracking data. \citet{sarlis2020sports} review many of these methods, including regression-based models for player efficiency metrics, clustering techniques for grouping players with similar playing styles, and rating systems that quantify individual contribution to team success. In practice, coaches and analysts often combine such player-level models with domain knowledge to support decisions about rotations, matchups, and roster construction.

Some work, including \citet{yang2015nba}, aggregates player statistics into team-level features to predict season outcomes, effectively creating a simple player-to-team pipeline. However, existing player-focused models typically concentrate on a single type of outcome (e.g., efficiency ratings or win shares), and most studies treat player-level prediction and team-level season forecasting as separate problems. There is comparatively little work that simultaneously (i) uses player box-score statistics to predict league recognition and role (e.g., awards or coarse positional groups), (ii) aggregates those same player-level statistics into team-level profiles, and (iii) compares these player-informed team models against strong, standings-based baselines under a unified evaluation protocol.

Our project is designed to fill this gap. We train player-level classifiers on season-long box-score features to predict All-Star selection, All-NBA/MVP–related honors, and coarse positional categories (Guard / Wing / Big), aggregate player statistics back to the team level to construct player-informed team features, and place these models alongside a purely standings-based baseline and an automatically tuned family of regression pipelines. Evaluating all models on the same task---predicting team win percentage and resulting standings over held-out seasons---allows us to quantify when player-informed models meaningfully improve upon historical win–trend baselines and to connect individual player profiles to expected team performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tasks and evaluation}
\label{sec:notation}

We organize our project around four related prediction tasks:

\begin{enumerate}
  \item a \emph{standings-only} baseline that forecasts team win percentage from historical wins and losses;
  \item three \emph{player-level} classification tasks (All-Star selection, major awards, and coarse position);
  \item a \emph{team-level} regression model that predicts team win percentage from richer team box-score and trend features; and
  \item an \emph{auto-tuned ranking model} that searches over feature groups, preprocessing strategies, and model families to maximize team-ranking accuracy.
\end{enumerate}

The team-level tasks (1, 3, and 4) all operate at the season-by-team level but differ in how much information beyond past wins they use. The player-level tasks (2) operate on individual season-by-player rows and are evaluated independently. Below we describe the targets, data splits, and evaluation metrics used for each group of tasks.

\subsection{Team-level win-percentage prediction from standings only}

For the standings-only baseline, we work directly with season-level win–loss records. For each team $t$ and season $s$, we define the realized winning percentage
\[
\text{WIN\_PCT}_{t,s}
=
\frac{\text{WINS}_{t,s}}
     {\text{WINS}_{t,s} + \text{LOSSES}_{t,s}},
\]
and construct a feature vector $\mathbf{x}^{\text{stand}}_{t,s}$ from team-specific historical trends: previous-season win percentage, three- and five-year rolling averages, and a short-term trend in the rolling average. A linear regression model $f_\theta$ is trained to predict
\[
\hat{y}_{t,s} = f_\theta(\mathbf{x}^{\text{stand}}_{t,s}),
\]
interpreted as a forecast of $\text{WIN\_PCT}_{t,s}$.

We use team–season summaries from the 2004--2005 through 2024--2025 seasons (630 team–season observations). The two most recent seasons (2023--2024 and 2024--2025) are held out for evaluation; all earlier seasons are used for training. Trend features are computed on the full panel, but model fitting uses only the training seasons. We evaluate using standard regression and correlation metrics computed separately for each held-out season: MAE, RMSE, $R^2$, and Pearson and Spearman correlation between predicted and realized win percentage.

\subsection{Player-level prediction tasks}

We consider three player-centric classification tasks, each defined on season-by-player rows with box-score features and temporally separated train/test splits.

\paragraph{All-Star selection.}
For each player $p$ and season $s$, we build a feature vector $\mathbf{x}^{\text{AS}}_{p,s}$ from counting and rate statistics (minutes, shooting volume and efficiency, rebounding, playmaking, and defensive stats), together with one-hot encoded position and team indicators. The target label $y^{\text{AS}}_{p,s} \in \{0,1\}$ indicates whether the player received an All-Star designation that season, obtained by parsing the awards string. We train on earlier seasons and evaluate on the last three.

\paragraph{All-NBA / MVP awards.}
The second player-level task predicts whether a player appears in major end-of-season awards. The feature vector $\mathbf{x}^{\text{MVP}}_{p,s}$ uses all numeric box-score features, and the binary label $y^{\text{MVP}}_{p,s}$ is set to 1 if the awards string contains any All-NBA or MVP-related marker and 0 otherwise. To handle extreme class imbalance, we learn an XGBoost classifier with a class-weighting term derived from the positive-to-negative ratio in the training data.

\paragraph{Coarse position classification.}
The third player-level task maps each player-season to one of three coarse position groups:
Guard (PG/SG), Wing (SF), or Big (PF/C). We derive a label $y^{\text{pos}}_{p,s} \in \{\text{Guard}, \text{Wing}, \text{Big}\}$ by collapsing raw position strings and discarding ambiguous entries. The feature vector $\mathbf{x}^{\text{pos}}_{p,s}$ includes both raw box-score quantities and engineered features such as per-36-minute statistics, shooting rates, assist-to-turnover ratio, and a usage proxy. We again train on earlier seasons and test on the most recent seasons (starting in 2023).

For all three player-level tasks, we evaluate classifiers using overall accuracy on held-out seasons, per-class precision/recall/F1, and confusion matrices. For the multi-class position task, we also compare against a majority-class baseline.

\subsection{Team-level ranking prediction with rich features}

Beyond the standings-only baseline, we train models that use richer season-level team statistics to predict team strength. For each team $t$ and season $s$, we assemble a feature vector $\mathbf{x}^{\text{team}}_{t,s}$ from box-score totals, efficiency metrics, and trend features (described below) and train a regression model $g_\phi$ to output a scalar score
\[
\hat{z}_{t,s} = g_\phi(\mathbf{x}^{\text{team}}_{t,s}),
\]
which we use as a proxy for team quality. Within each season, teams are ranked by sorting $\hat{z}_{t,s}$ in descending order. We reuse the same temporal split as in the standings-only baseline: 2023--2024 and 2024--2025 are held out, and all prior seasons are used for training. Feature engineering (rolling box-score averages and win-percentage trends) is performed on the full panel, but model fitting never accesses the held-out seasons.

Since the main objective is to recover the correct \emph{ordering} of teams within a season, we evaluate team-level predictions using rank-based metrics. Let $\mathrm{RANK}^{\mathrm{true}}_{t,s}$ denote the rank of team $t$ in season $s$ when teams are sorted by realized winning percentage (1 = best), and let $\mathrm{RANK}^{\mathrm{pred}}_{t,s}$ be the rank when sorted by $\hat{z}_{t,s}$. For each season $s$ with $N_s$ teams, we compute exact rank accuracy and the fractions of teams whose predicted rank is within one or two places of the truth:
\begin{align*}
\mathrm{ExactAcc}_s
&=
\frac{1}{N_s}\sum_t
\mathbb{1}\bigl[
\mathrm{RANK}^{\mathrm{pred}}_{t,s}
=
\mathrm{RANK}^{\mathrm{true}}_{t,s}
\bigr],\\[0.5em]
\mathrm{Within1}_s
&=
\frac{1}{N_s}\sum_t
\mathbb{1}\bigl[
\lvert\mathrm{RANK}^{\mathrm{pred}}_{t,s}
-
\mathrm{RANK}^{\mathrm{true}}_{t,s}\rvert
\le 1
\bigr],\\[0.5em]
\mathrm{Within2}_s
&=
\frac{1}{N_s}\sum_t
\mathbb{1}\bigl[
\lvert\mathrm{RANK}^{\mathrm{pred}}_{t,s}
-
\mathrm{RANK}^{\mathrm{true}}_{t,s}\rvert
\le 2
\bigr].
\end{align*}
We report each metric for the two held-out seasons separately and also average them across seasons. Figure~\ref{fig:rank-scatter} visualizes predicted versus actual rankings for the held-out seasons under the best-performing configuration.

\begin{figure*}
\centering
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{season_2023_2024_rank_scatter}
    \caption{2023--2024 season.}
    \label{fig:rank-2324}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{season_2024_2025_rank_scatter}
    \caption{2024--2025 season.}
    \label{fig:rank-2425}
\end{subfigure}
\caption{
Predicted versus actual team rankings for the two held-out seasons.
Each point corresponds to a single team; the dashed diagonal indicates
perfect agreement between predicted and true rankings.
}
\label{fig:rank-scatter}
\end{figure*}

\subsubsection{Feature groups}

To structure the model search space, we organize candidate
team-level predictors into six feature groups and randomly sample combinations of these groups during model search:
\begin{itemize}
  \item \textbf{basic\_box}: core box-score totals (PTS, FGA, FTA, TRB, AST, STL, BLK, TOV, PF),
  \item \textbf{basic\_eff}: efficiency ratios (TS\_PCT, EFG\_PCT, AST\_TO\_RATIO) and previous-season win percentage (PREV\_WIN\_PCT),
  \item \textbf{four\_factors}: the “Four Factors” metrics (effective field-goal percentage, turnover factor, offensive rebounding factor, and free-throw rate),
  \item \textbf{rolling\_box}: 3-year rolling averages of key box-score totals,
  \item \textbf{full\_box}: the full traditional box-score set, and
  \item \textbf{trend}: win-percentage trend features (three- and five-year rolling averages and a short-term trend).
\end{itemize}
Each sampled configuration selects a subset of these groups, and the corresponding union of columns is used as $\mathbf{x}^{\text{team}}_{t,s}$.

\subsubsection{Models, preprocessing, and hyperparameter search}

We perform a random search over model families, feature-group combinations, and preprocessing strategies. Model families include gradient boosting, random forests, LightGBM, XGBoost, ridge and lasso regression, support vector regression, $k$-nearest neighbors, and small multilayer perceptrons. For each model, we sample hyperparameters from predefined ranges.

Before model fitting, we also sample a preprocessing strategy from a small set of options: standardization, robust scaling, min–max scaling, or no scaling. For a given configuration, we:

\begin{enumerate}
  \item build the selected feature matrix on all training seasons,
  \item fit the chosen preprocessing transform (if any) and model on the training data, and
  \item evaluate the resulting model on the held-out seasons using the rank-based metrics above.
\end{enumerate}

We record all configurations and retain those that achieve strong ranking performance (e.g., high exact and Within1 accuracy) on the two held-out seasons for further analysis. Additional diagnostic summaries of the search space are provided in Appendix~\ref{app:model-search}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project description and design process}
\label{sec:project}

This section describes the concrete data we use and the models we implement
for each task, along with the main design choices that led to the final
pipeline.

\subsection{Data sources and preprocessing}

We work with season-level NBA team and player statistics spanning roughly
the 2004--2005 through 2024--2025 regular seasons. All data come from
publicly available box-score and standings tables and are stored in
CSV files processed with Python.

\paragraph{Team-level data.}
At the team level we use two types of tables:
\begin{itemize}
    \item \textbf{Win--loss summaries} (\texttt{Team\_stats/WL}): for each team and season,
    we have wins, losses, and a team abbreviation. From these we compute
    the realized winning percentage $\text{WIN\_PCT}$ and trend features:
    previous-season win percentage, three- and five-year rolling averages,
    and a short-term trend for each franchise.
    \item \textbf{Expanded team summaries}
    (\texttt{Team\_stats/all\_seasons\_team\_summary.csv}): these tables
    contain traditional box-score totals, shooting percentages, and win--loss records for every
    team-season. We use these to derive efficiency metrics (true
    shooting percentage, effective field-goal percentage, assist-to-turnover
    ratio) and the “Four Factors,” as well as three-year rolling averages of key stats.
\end{itemize}

We clean numeric columns by replacing infinite values
with \texttt{NaN} and dropping rows with missing wins or losses. Seasons
are indexed by both their string form (e.g., 2023--2024) and an integer
end year (e.g., 2024) to support temporal splitting and rolling-window
computations.

\paragraph{Player-level data.}
At the player level we use a single long panel
(\texttt{all\_stats/player\_20years.csv}) containing one row per
player-season with per-game box-score stats, minutes, age, team, raw
position string, an awards string, and a season identifier.

For preprocessing we:
\begin{itemize}
  \item filter out rows with zero games played,
  \item drop the aggregated ``TOT'' rows for players traded mid-season,
  \item derive season start and end years from the season string, and
  \item apply a minimum minutes-played threshold (e.g., 500 minutes) when learning player positions.
\end{itemize}

For the position classification task we engineer additional features:
per-36-minute versions of core stats, per-36 shooting volume, and
ratio-based descriptors such as three-point and free-throw rate,
assist-to-turnover ratio, and a usage proxy based on scoring attempts.

\paragraph{Label construction.}
The three player-level tasks require different labels:
\begin{itemize}
  \item \textbf{All-Star label}: parse the awards string for the substring
  \texttt{"AS"} and set a binary label accordingly.
  \item \textbf{All-NBA / MVP label}: define \texttt{LABEL\_ALLNBA\_MVP}
  by checking whether an awards string contains markers related to All-NBA
  teams or MVP voting.
  \item \textbf{Coarse position label}: map raw position strings into
  Guard (PG/SG), Wing (SF), and Big (PF/C), dropping ambiguous entries.
\end{itemize}
All temporal splits use these season identifiers: earlier seasons for training and the most recent ones as held-out test sets.

\subsection{Model variants by task}

We implement different model families tailored to each of the four main
tasks introduced in Section~\ref{sec:notation}.

\paragraph{Task 1: Standings-only team baseline.}
We use a linear regression model trained on team-level trend features
derived from historical win--loss records (previous-season win
percentage, three- and five-year rolling averages, and a short-term
trend). Features are standardized before training. The model is trained on
all but the last two seasons and evaluated on 2023--2024 and 2024--2025.

\paragraph{Task 2: Player-level classification.}
We train three separate classifiers on the long player panel.

\begin{itemize}
  \item \textbf{All-Star prediction}:
  a \texttt{GradientBoostingClassifier} with 400 trees and depth 3. The
  input combines numeric box-score statistics with one-hot encoded team
  and position indicators via a column-wise preprocessing pipeline.

  \item \textbf{All-NBA / MVP prediction}:
  an \texttt{XGBClassifier} that ingests all numeric features except
  identifiers. To address class imbalance, we compute a
  \texttt{scale\_pos\_weight} from the positive rate in the training set
  and pass it to XGBoost.

  \item \textbf{Coarse position classification}:
  an XGBoost multi-class classifier on the enriched feature set with
  pre-tuned hyperparameters (e.g., 600 trees, depth 4, learning rate
  0.1). We filter out low-minute seasons and standardize features before
  training.
\end{itemize}

Each classifier is evaluated on a temporally separated test set and
compared to simple baselines such as always predicting the majority
class (for positions).

\paragraph{Task 3: Player-to-team aggregation model.}
To connect player performance back to team outcomes, we build a
team-level panel by aggregating player statistics up to the team-season
level and merging them with official team summaries.

We group player rows by season and team, summing counting stats and
averaging age. From these we recompute team-level shooting percentages,
true shooting percentage, effective field-goal percentage, and
assist-to-turnover ratio. We then merge this player-derived
table with the official team summary table and compute win-percentage
trend features as before. On this merged panel we train a
\texttt{RandomForestRegressor} on all seasons except the last two,
targeting team win percentage and evaluating on 2023--2024 and 2024--2025.

\paragraph{Task 4: Auto-tuned team ranking model.}
Finally, we construct an automatic model-search pipeline on the expanded
team summary table. For each random configuration, we:
\begin{enumerate}
  \item sample one or more feature groups from the six groups
  in Section~\ref{sec:notation} (never using trend features alone);
  \item build the corresponding feature matrix on all training seasons;
  \item sample a preprocessing strategy from
  \{\texttt{StandardScaler}, \texttt{RobustScaler}, \texttt{MinMaxScaler}, none\};
  \item sample a model family and its hyperparameters from a pool that includes LightGBM, XGBoost, random forests, gradient boosting,
  ridge and lasso regression, support vector regression, $k$-NN,
  simple multilayer perceptrons, and a few voting ensembles; and
  \item fit the model on all non-held-out seasons and evaluate its ranking
  accuracy on the two held-out seasons using the metrics in
  Section~\ref{sec:notation}.
\end{enumerate}
We track the performance and configuration of each run and export the detailed predictions of the global best model for qualitative inspection.

\subsection{Design process}

We approached the project as an iterative design process that gradually
increased model complexity while keeping temporally realistic
evaluation.

\paragraph{Phase 1: Simple team baselines.}
We started with the most transparent team-level baselines: predicting
future team win percentage from only past win--loss records. Early
experiments showed that a linear model on a small set of historical
win-percentage statistics already explains a large fraction of variance
in future win percentage. This motivated treating a standings-only trend
model as a central baseline and fixing a strict temporal split.

\paragraph{Phase 2: Player-level modeling.}
We then moved to player-level tasks to understand how much signal is
available in individual statistics. We chose All-Star, major awards, and
coarse position because they are tied to real basketball concepts and
allow us to quantify how well gradient boosting and XGBoost models can
recover expert decisions and positional roles from box-score data alone.
Severe class imbalance for awards led us to add explicit class weighting
and to monitor per-class metrics rather than only overall accuracy.

\paragraph{Phase 3: From players back to teams.}
Next, we asked whether aggregating player stats back to teams would help
predict future team performance. Initial high-dimensional designs that
concatenated player vectors proved fragile and hard to interpret, so we
settled on a simpler aggregation that recomputes familiar team-level
metrics and feeds them into a tree-based regressor. This provided a
stable bridge between the player and team tasks.

\paragraph{Phase 4: Automated model and feature search.}
Finally, to avoid hand-picking feature sets and models at the team
level, we built an auto-optimizer that samples feature-group
combinations, preprocessing methods, and model hyperparameters and
evaluates them against the same held-out seasons using rank-based
metrics. This framework let us quantify variability across reasonable
model choices, identify robustly strong configurations, and position our
best model relative to the simpler standings-only baseline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation, user testing, and iteration}
\label{sec:testing}

Even though our project is not deployed, we still treat evaluation as a
user-centered process. Our target users are basketball fans and analysts
who want models that are both accurate and easy to interpret in
basketball terms (wins, rankings, awards, positions).

\subsection{Train--test split based on time} 

To mimic realistic forecasting, all tasks use time-based splits instead
of random splits within a season:

\begin{itemize}
  \item \textbf{Team-level tasks (1, 3, 4):} we train on seasons up to
  2022--2023 and test on 2023--2024 and 2024--2025. Trend features
  (e.g., rolling averages) use only past seasons for each team.
  \item \textbf{Player-level task (2):} we train on earlier seasons and
  hold out recent ones. For position prediction, we again train up to
  2022--2023 and test on later seasons.
\end{itemize}

This protocol ensures that models never see future information and
matches how fans would use the system in practice.

\subsection{Metrics and iteration by task}

We choose metrics that match what our users care about and use them to
guide iteration.

For the standings-only baseline and the player-aggregated team model, we
track MAE, RMSE, $R^2$, and correlations on the two held-out seasons.
After seeing that a small set of win–loss trend features already gave
low error and high $R^2$, we fixed this trend model as our main
baseline model.

For the team model, our main goal is to get the rankings
right. We therefore focus on ExactAcc, Within1, and Within2, and use a
combined score that emphasizes Within1 as the optimizer’s objective. We
keep configurations that consistently improve these ranking metrics over
the standings baseline.

For All-Star and MVP/All-NBA prediction, early models with only accuracy
looked good overall but almost never predicted awards. Confusion
matrices showed that the model predicted “no award” for almost everyone.
We then added class weights, monitored positive-class recall and F1, and
adjusted thresholds until we achieved a better precision–recall tradeoff
on the award classes.

For position prediction, we compare against an “always Guard” baseline
and study confusion matrices. Guards and Bigs are learned reasonably
well, but Wings are harder to separate. This pushed us to add
per-36-minute rebounding, shooting volume, and usage-like features to
make wing profiles more distinct.

Finally, for the player-to-team aggregation pipeline, we compare MAE,
RMSE, and correlations directly to the standings-only baseline. More
complex, high-dimensional variants did not clearly improve these
metrics and were harder to explain, so we kept a simpler aggregation
that sums player stats and recomputes standard team-level efficiencies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Findings}
\label{sec:findings}

We summarize the main empirical findings from our four tasks:
(i) team-level forecasting from standings history,
(ii) team-level forecasting from rich box-score features,
(iii) player-level prediction tasks, and
(iv) automated model and feature search.

\subsection{Team-level forecasting from W/L history}

Using only team wins, losses, and simple trend features
(previous-season win percentage and multi-year rolling averages),
the baseline linear model already predicts win percentage very well on
the two most recent seasons.
For the 2023--2024 hold-out season, the model attains
$\text{MAE} \approx 0.032$, $\text{RMSE} \approx 0.043$,
$R^2 \approx 0.93$, with Pearson and Spearman correlations above
$0.92$.
For 2024--2025, performance remains strong
($\text{MAE} \approx 0.040$, $\text{RMSE} \approx 0.044$,
$R^2 \approx 0.92$).
A “last-year plus trend’’ baseline already captures much of the signal
in team outcomes.

\subsection{Player-level prediction tasks}

On the player side, we obtain three complementary findings.

\paragraph{All-Star prediction.}
A gradient boosting classifier using per-player box-score features,
minutes, age, and one-hot team/position indicators achieves
$98.2\%$ accuracy on held-out seasons.
Despite strong class imbalance (about $3\%$ of players are All-Stars),
the model reaches recall $0.88$ and F1-score $0.75$ for the positive
class, recovering most true All-Stars with relatively few false
positives.

\paragraph{All-NBA / MVP prediction.}
An XGBoost classifier trained to detect whether a player earns any
All-NBA or MVP-related award achieves $99.1\%$ test accuracy.
For the small positive class (about $1.5\%$ of players), the model
attains recall $0.90$ and F1-score $0.76$, suggesting that simple
season-long box-score profiles are highly informative about
end-of-season award outcomes.

\paragraph{Position classification.}
For three-way position labels (Guard / Wing / Big), an XGBoost
multi-class model with engineered per-36 and rate statistics achieves
$76.6\%$ test accuracy, substantially above the majority baseline
($44.4\%$, always predicting ``Guard'').
Performance is strongest for Guards and Bigs (F1 $\approx 0.82$--$0.84$),
and weakest for Wings (F1 $\approx 0.45$), reflecting that Wings
overlap statistically with both backcourt and frontcourt roles.

\subsection{From players back to teams}

Aggregating player-level statistics up to team-season level and then
predicting team win percentage yields performance comparable to the
trend-only baseline.
Using a Random Forest on trend features derived from the
player-aggregated data, we obtain, for 2023--2024,
$\text{MAE} \approx 0.041$, $\text{RMSE} \approx 0.054$,
$R^2 \approx 0.89$, and for 2024--2025,
$\text{MAE} \approx 0.046$, $\text{RMSE} \approx 0.056$,
$R^2 \approx 0.88$, with Pearson and Spearman correlations above
$0.93$.
Aggregating player trajectories back to the team level produces realistic forecasts, but does not easily
outperform a carefully tuned standings-based baseline on this dataset.

\subsection{Best team-level ranking model from automated search}

The automated search identifies a gradient boosting regressor using
four feature groups:
\texttt{basic\_box}, \texttt{basic\_eff}, \texttt{rolling\_box}, and
\texttt{trend}.
Evaluated as a ranking model on the two held-out seasons, this
configuration achieves:
\begin{itemize}
    \item 2023--2024: $83.3\%$ exact rank accuracy, $83.3\%$ within~1,
          and $96.7\%$ within~2;
    \item 2024--2025: $40.0\%$ exact rank accuracy, $76.7\%$ within~1,
          and $93.3\%$ within~2.
\end{itemize}
Averaged across the two seasons, the model reaches $63.3\%$ exact rank
accuracy, $78.3\%$ within~1, and $91.7\%$ within~2, substantially
improving on naive baselines that rely only on last year's standings.

\subsection{Interpretability and explanatory power}

Qualitatively, the different components of the pipeline offer different
kinds of explanations:

\begin{itemize}
    \item The W/L-trend baseline is extremely simple and transparent:
          it encodes the belief that teams tend to regress slowly toward
          their historical average.
    \item The player-level award and position models expose which
          statistical profiles are associated with “star’’ seasons and
          specific roles, making it easy to reason about why a given
          player is flagged as All-Star, All-NBA/MVP, or a particular
          position type.
    \item The player-to-team aggregation closes the loop by showing how
          the collection of individual contributions translates into
          expected team success, even if it does not always beat the
          standings-only baseline numerically.
    \item The final gradient-boosting ranking model balances accuracy
          and interpretability at the team level: its feature groups
          correspond to familiar concepts (box-score totals, efficiency,
          long-term trends), which can be inspected when explaining why
          a team is projected higher or lower than its previous record.
\end{itemize}

Overall, simple standings-based models are competitive, but carefully
engineered season-level features and player-centric analyses narrow the
remaining performance gap and provide richer, more intuitive
explanations for forecasted outcomes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}

Our project aims to build a forecasting pipeline that is both
reasonably accurate and understandable for basketball fans and analysts.
The findings support three main contributions:
(i) quantifying how far simple standings-based models can go,
(ii) showing where more complicated team-level modeling and automated search add
value, and
(iii) demonstrating how player-level models provide complementary,
interpretable views of team strength.

\subsection{What simple baselines already capture}

A first takeaway is how strong the historical-standings baseline is.
Using only past win percentage and trend features, the baseline
model achieves $\text{MAE} \approx 0.03$--$0.04$ and
$R^2 \approx 0.92$--$0.93$ on the two most recent seasons, with
correlations above $0.92$.
Much of future performance change can be explained by
recent team performance alone: rosters, coaching staffs, and
organizational strategies tend to change gradually, so win totals does not change dramatically for most teams.
For users who just want a quick and straightfoward  forecast, this
type of model is sufficient.

\subsection{Where harder team modeling truly helps}

The automated model and feature search demonstrates that carefully
chosen team-level features can further improve ranking accuracy.
The gradient boosting model built on
\texttt{basic\_box}, \texttt{basic\_eff}, \texttt{rolling\_box}, and
\texttt{trend} features attains
$63.3\%$ exact rank accuracy and more than $90\%$ within--2 across the
two held-out seasons.
This is a clear gain over relying only on last year’s record: the model
uses information about scoring volume, efficiency, rebounding,
playmaking, and long-term trends to rank teams more precisely.

However, these improvements have limits. In some seasons, the optimized model does a much better job at predicting the exact rankings, but in others it only shuffles teams within the same general tier.
This suggests a design principle for fan-facing predicting tools: adding model complexity is only  helpful when the features are clear and interpretable rather than pure feature engineering that is not related to basketball at all.

\subsection{What player-level models add}

The player-level tasks point to different side of the problem.
Our All-Star and All-NBA/MVP classifiers reach high accuracy and strong recall despite the labels being heavily imbalanced, which shows that basic box-score stats already contain enough information to identify most “star seasons.”
The position classifier also reveals clear patterns: Guards and Bigs live in fairly distinct statistical spaces, while Wings are more mixed and sit between backcourt and frontcourt styles.
These models do not directly improve team-level $R^2$, but they clarify
how the system “thinks’’ about individual roles and star power.

When we aggregate player statistics back to the team level, we get performance that is roughly on par with the standings baseline. More importantly, this approach highlights where the baseline can fail: when a roster changes dramatically or when a team loses stars to injuries.
In those settings, being able to trace a forecast back to specific
players, aging curves, and usage patterns gives analysts and fans a more
satisfying narrative than team-only regressions can provide.

\subsection{Implications for fan-facing analytics}

From a design perspective, the comparison between models illustrates how
to balance simplicity, accuracy, and transparency:

\begin{itemize}
    \item The standings baseline is easy to explain and can serve as a
          robust default method.
    \item The richer feature-complex team model improves
          ranking accuracy but still using similar statistics.
    \item The player-level models and player-to-team aggregation give
          better interpretability about \emph{who} drives the forecast, even the overall win prediction is numerically close to the
          baseline.
\end{itemize}

For fan- and analyst-facing systems, the most useful designs are not
necessarily the most complex models, but those that combine:
(i) a strong, simple baseline,
(ii) modest accuracy gains from richer features, and
(iii) player-centric explanations that align with how people already
talk about the game.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and future work}
\label{sec:limitations}

Our study has several limitations that are important to keep in mind
when interpreting the results and thinking about generalization beyond
our specific setting.

\subsection{Data and modeling assumptions}

First, we work entirely within the NBA and rely primarily on
traditional box-score data plus simple roster metadata.
We do not incorporate richer tracking data, play-by-play information,
lineup context, or opponent-adjusted metrics.
As a result, both the team-level and player-level models may miss
important aspects of defensive impact, spacing, on/off effects, and
scheme-specific roles that are not well captured by box-score
statistics alone.

Second, the player evolution components assume relatively smooth aging
curves and largely linear dynamics in standard features.
In reality, player trajectories can be highly nonlinear and
injury-prone, with abrupt jumps due to role changes, coaching shifts,
or major trades.
Our player-to-team aggregation further assumes that team strength is
approximately the sum of individual contributions after basic filtering
(e.g., minutes thresholds), and does not explicitly model interaction
effects such as lineup fit or coaching strategy.

Third, our label choices and thresholds introduce their own biases.
All-Star and All-NBA/MVP indicators are influenced by media narratives,
fan voting, and award rules, and the extreme class imbalance means that
small absolute errors can translate into large swings in precision and
recall.
Similarly, position labels are collapsed into three broad groups,
obscuring finer-grained role distinctions that matter in modern
positionless basketball.

\subsection{Evaluation constraints}

On the evaluation side, we are constrained by the relatively small
number of independent seasons.
Even with rolling train--test splits, there are only so many
non-overlapping temporal splits available.
This makes it difficult to obtain very tight confidence intervals on
performance differences and raises the risk that some conclusions are
specific to the recent era of NBA rules and play style.

For the team-ranking task we evaluate on the two most recent seasons,
which provides a realistic forecasting scenario but also means that our
best-performing configuration is tuned to a small number of
out-of-sample years.
Likewise, our informal “user tests’’ involve a small, convenience
sample of classmates; they provide useful qualitative feedback but not a
controlled HCI study.

Moreover, for the auto-tuned team-ranking model we implicitly use the
same two held-out seasons both to select the best-performing
configuration and to report its final performance. Although the test
seasons are never used for training the underlying models, this reuse
of the evaluation set as a model-selection objective introduces a mild
form of leakage: our reported metrics for Task~4 are likely to be
somewhat optimistic, and a truly unbiased estimate of generalization
would require fixing a configuration first and then evaluating it on
additional, unseen future seasons.

\subsection{Directions for future work}

These limitations suggest several directions for improvement:

\begin{itemize}
    \item \textbf{Richer and more granular data.}
          Incorporating tracking data, play-by-play logs,
          opponent-adjusted impact metrics, and explicit injury or
          availability information could help the models better capture
          defense, spacing, and lineup context.
    \item \textbf{More flexible but structured models.}
          Exploring nonlinear or hierarchical models for player
          trajectories and team strength, while constraining them with
          interpretable feature sets, could relax smooth-aging and
          additivity assumptions without sacrificing transparency.
    \item \textbf{Richer evaluation and user studies.}
          Extending the evaluation to additional seasons, other
          leagues, or other sports would clarify how robust our design
          choices are.
          A more systematic user study with analysts and different types
          of fans could measure how model explanations affect trust and
          perceived usefulness.
    \item \textbf{Interactive, user-facing designs.}
          Building an interactive interface (e.g., allowing users to
          adjust hypothetical trades or injuries and see updated
          forecasts) would test how well the pipeline supports real
          analytical workflows and what additional explanations are
          needed in practice.
\end{itemize}

Overall, our work should be viewed as a first step toward combining
simple standings-based baselines, richer team-level features, and
player-level reasoning in a way that is both quantitatively reasonable
and accessible to basketball audiences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusion}

We set out to ask a seemingly simple question: when is a very simple
team-level model “good enough’’ for predicting NBA performance, and
when is it worth investing in a more complex, player-centric pipeline?

Our results point to three main takeaways.

First, historical standings by themselves are a remarkably strong
baseline.
A large share of the variance in future win percentage can be explained
by past wins and simple trend features, especially for teams with
relatively stable rosters and coaching situations.
For many quick-turnaround use cases, this kind of standings-based model
offers a lightweight, easy-to-communicate forecast.

Second, player-based pipelines matter most when teams undergo structural
change.
When rosters shift substantially, a model that explicitly projects
individual player trajectories and aggregates their contributions to the
team level can improve ranking accuracy and, just as importantly, offer
richer explanations of \emph{why} a team is expected to rise or fall.
Being able to trace a forecast back to specific players, aging curves,
and usage patterns gives analysts and fans a more satisfying narrative
than team-only regressions can provide.

Third, the choice between these models should be driven by both the
structure of the forecasting problem and the needs of the intended
users.
In settings where data are limited, rosters are stable, or users mainly
need a rough directional signal, a simple standings-based approach may
be sufficient.
In settings where stakeholders care about transparency, counterfactual
scenarios (e.g., trades, injuries), or storytelling around players, a
player-centric design offers clear added value even if raw accuracy
improves only modestly.

More broadly, our project illustrates how sports analytics tools can be
designed to balance simplicity, interpretability, and responsiveness to
change.
By putting a team-level baseline and a player-level pipeline side by
side on the same forecasting task, we highlight the trade-offs that
practitioners face when choosing model complexity, and show how
user-centered considerations can guide that choice rather than accuracy
alone.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acknowledgements}
We thank the open basketball-reference and NBA Stats communities for
making historical data accessible, and the Northwestern MSCS program
for general support of this work.
\end{acknowledgements}

% ==== 参考文献：保持 A&A 的 aa.bst，author–year 风格 ====
\bibliographystyle{apalike}
\bibliography{refs}

\begin{appendix}
\clearpage
\onecolumn

\section{Additional team-level model search results}
\label{app:model-search}

% \begin{table*}
\begin{table}[h!]
\centering
\caption{Representative high-performing configurations from the random search over team-level ranking models. All listed runs achieve test-time exact rank accuracy $\mathrm{ExactAcc} \ge 0.40$ on average across the 2023--2024 and 2024--2025 seasons.}
\label{tab:model-search}
\scriptsize
\begin{tabular}{llcc}
\hline\hline
Feature groups & Model & ExactAcc & Within1 \\
\hline
basic\_box + trend
  & XGBoost (RobustScaler)
  & 0.633 & 0.783 \\

basic\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.617 & 0.767 \\

basic\_box + trend
  & LightGBM (no scaling)
  & 0.600 & 0.767 \\

basic\_box + trend
  & Random Forest (no scaling)
  & 0.583 & 0.750 \\

basic\_box + trend
  & MLP regressor (StandardScaler)
  & 0.450 & 0.700 \\

basic\_box + full\_box + trend
  & XGBoost (RobustScaler)
  & 0.617 & 0.783 \\

basic\_box + full\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.600 & 0.767 \\

basic\_box + full\_box + trend
  & LightGBM (StandardScaler)
  & 0.583 & 0.750 \\

basic\_box + four\_factors + trend
  & XGBoost (RobustScaler)
  & 0.600 & 0.783 \\

basic\_box + four\_factors + trend
  & Gradient Boosting (RobustScaler)
  & 0.583 & 0.767 \\

basic\_box + four\_factors + trend
  & Random Forest (no scaling)
  & 0.567 & 0.750 \\

basic\_box + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.600 & 0.767 \\

basic\_box + rolling\_box + trend
  & LightGBM (RobustScaler)
  & 0.583 & 0.767 \\

basic\_box + rolling\_box + trend
  & Random Forest (no scaling)
  & 0.550 & 0.733 \\

four\_factors + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.567 & 0.750 \\

four\_factors + rolling\_box + trend
  & Gradient Boosting (StandardScaler)
  & 0.550 & 0.733 \\

four\_factors + rolling\_box + trend
  & LightGBM (no scaling)
  & 0.533 & 0.733 \\

basic\_eff + trend
  & XGBoost (StandardScaler)
  & 0.517 & 0.717 \\

basic\_eff + trend
  & Gradient Boosting (StandardScaler)
  & 0.500 & 0.700 \\

basic\_eff + trend
  & Random Forest (no scaling)
  & 0.483 & 0.700 \\

basic\_eff + four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.517 & 0.717 \\

basic\_eff + four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.500 & 0.700 \\

basic\_eff + four\_factors + trend
  & LightGBM (StandardScaler)
  & 0.483 & 0.700 \\

full\_box + trend
  & XGBoost (RobustScaler)
  & 0.583 & 0.767 \\

full\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.567 & 0.750 \\

full\_box + trend
  & LightGBM (StandardScaler)
  & 0.550 & 0.750 \\

full\_box + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.583 & 0.767 \\

full\_box + rolling\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.567 & 0.750 \\

full\_box + rolling\_box + trend
  & Random Forest (no scaling)
  & 0.550 & 0.733 \\

basic\_box + basic\_eff + trend
  & XGBoost (StandardScaler)
  & 0.550 & 0.750 \\

basic\_box + basic\_eff + trend
  & Gradient Boosting (StandardScaler)
  & 0.533 & 0.733 \\

basic\_box + basic\_eff + trend
  & LightGBM (StandardScaler)
  & 0.517 & 0.733 \\

basic\_box + basic\_eff + four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.550 & 0.750 \\

basic\_box + basic\_eff + four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.533 & 0.733 \\

basic\_box + basic\_eff + four\_factors + trend
  & Random Forest (no scaling)
  & 0.517 & 0.717 \\

four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.500 & 0.717 \\

four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.483 & 0.700 \\

four\_factors + trend
  & Random Forest (no scaling)
  & 0.467 & 0.700 \\

trend only
  & Ridge regression (StandardScaler)
  & 0.433 & 0.667 \\

trend only
  & Lasso regression (StandardScaler)
  & 0.417 & 0.650 \\

trend only
  & SVR (StandardScaler)
  & 0.417 & 0.650 \\

trend only
  & KNN regressor (MinMaxScaler)
  & 0.400 & 0.633 \\
\hline
\end{tabular}
% \end{table*}
\end{table}

\end{appendix}

\end{document}
