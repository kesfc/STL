%-----------------------------------------------------------------------
%                                                                 aa.tex
% AA vers. 9.3, LaTeX class for Astronomy & Astrophysics
%-----------------------------------------------------------------------

\documentclass{aa}  

\usepackage{graphicx}
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{lscape}
\usepackage{placeins}

% ==== 超链接包 ====
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}

\title{Predicting NBA Team Performance: \\
From Historical Standings to Player-level Forecasts}

\author{
    Weihao Li\inst{1}\fnmsep\thanks{Corresponding author: wli@u.northwestern.edu}
    \and Shuhao Gao\inst{1}
    \and Shijie Chen\inst{1}
    \and Anbang Liu\inst{1}
}

\institute{
    McCormick School of Engineering, Northwestern University \\
    \email{\{WeihaoLi2027, ShuhaoGao2027, ShijieChen2027, AnbangLiu2027\}@u.northwestern.edu}
}

\date{Received December 8, 2025}

%-----------------------------------------------------------------
% ABSTRACT
%-----------------------------------------------------------------
\abstract
{In recent years, publicly available NBA play-by-play and box-score data have made it possible to build increasingly detailed models of team performance. At the same time, many forecasting systems still rely on simple team-level summaries such as past wins or point differential. We aim to compare a simple standings-based baseline with a player-centric forecasting pipeline for predicting future regular-season wins at the team level. We construct two models. The first directly regresses future team wins on historical win totals over the past two decades. The second learns generic player evolution patterns from a long-horizon panel of both retired and active players, uses these patterns to predict next-season box-score statistics for active players, and aggregates player-level projections into team-level features. Across held-out seasons, the player-based pipeline improves mean absolute error relative to the standings baseline, especially for teams undergoing large off-season roster changes. For more stable teams, historical standings are surprisingly competitive and sometimes match the player-based forecasts. Our results suggest that simple team-level baselines remain strong when rosters are stable, but player-level modeling adds value in high-turnover regimes and provides a more interpretable connection between roster moves and expected team performance.}

\keywords{basketball --
          sports analytics --
          machine learning --
          time series forecasting --
          player development}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In this project, we study how well different approaches can predict NBA team performance. Our goal is straightforward: given past data, can we forecast how many games each team will win in a future season? To explore this question, we compare two models that rely on fundamentally different sources of information.

The first model is a simple team-level baseline that uses only historical standings. For any pair of seasons within the past twenty years, we examine how a team's previous win total relates to its win total in another season. This provides a direct, standings-based method for predicting future performance without considering player-level factors.

The second model takes a player-centered perspective. We collect long-term statistics for both retired and still-active NBA players and train a model to learn how player performance tends to evolve over time. Using these learned patterns, we predict next-season box-score statistics for current active players and then aggregate the projected player output to estimate each team’s future win total.

By comparing these two approaches, we aim to understand when a simple
standings-based baseline is sufficient and when a more detailed, player-based forecasting pipeline provides an advantage, particularly in seasons affected by significant roster changes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prior Literature}

\subsection{Team-level rating and expectation models}

A large body of work in sports analytics has focused on predicting team performance using aggregate team-level statistics. One influential family
of models is the \emph{Pythagorean expectation}, which estimates a team's
theoretical winning percentage from points scored and points allowed via
a power-law relationship. Originally proposed for baseball and later
adapted to basketball, this idea underlies many simple baselines that
relate scoring margins to wins and losses
\citep[see, e.g.,][]{oliver2004basketball,sarlis2020sports}.
These models provide strong, easy-to-interpret benchmarks but depend
solely on aggregate scoring margins and do not incorporate information
about individual players.

Another common approach is to model team strength with rating systems
inspired by Elo. In basketball applications, Elo-style ratings are updated
after each game based on the result, home-court advantage, and the margin
of victory, and then used to forecast future game and series outcomes.
Public-facing systems such as FiveThirtyEight's NBA model illustrate how
dynamic ratings can track changes in team strength over a season and
provide reasonably accurate probabilistic predictions for both games and
playoff series \citep{silver2015elo}. Together, Pythagorean and Elo-style
systems represent static or quasi-static team-level baselines that are
closely related to our first model, which relies on historical team wins
and standings. However, because these methods operate on aggregate team
outcomes, they are inherently limited in their ability to anticipate
abrupt changes in performance driven by roster turnover or player
development.

\subsection{Machine learning for NBA game and season prediction}

Beyond analytic formulas and rating systems, many studies have applied
machine learning to predict basketball results using team-level features.
Early work by \citet{loeffelholz2009nba} used neural networks to predict
single-game NBA outcomes from box-score statistics and contextual
variables, demonstrating that nonlinear models can capture interactions
between basic team statistics. More recent surveys review a wide range of
approaches, including logistic regression, support vector machines,
tree-based ensembles, and deep neural networks, and typically find that
machine-learning models outperform simpler statistical baselines when
sufficient historical data are available \citep{sarlis2020sports}.

Researchers have also moved from game-level prediction to season-level
tasks, such as forecasting a team's final win total or playoff
qualification. For example, \citet{yang2015nba} regress regular-season
wins on team-level and aggregated player statistics to study which
factors are most predictive of team success. These season-level models
again treat each team as the unit of analysis and usually rely on summary
statistics from the current or previous season.

\subsection{Player-level prediction and its link to team performance}

Complementary to team-level approaches, a growing literature studies
player evaluation and performance prediction using detailed box-score and
tracking data. \citet{sarlis2020sports} review many of these methods,
including regression-based models for player efficiency metrics,
clustering techniques for grouping players with similar playing styles,
and rating systems that quantify individual contribution to team success.
In practice, coaches and analysts often combine such player-level models
with domain knowledge to support decisions about rotations, matchups, and
roster construction.

Some work, including \citet{yang2015nba}, aggregates player statistics
into team-level features to predict season outcomes, effectively creating
a simple player-to-team pipeline. However, existing player-focused models
typically train and evaluate on overlapping time periods for the same set
of players, and they rarely combine long-run player evolution with
explicit team-level baselines.

In contrast, our project is designed to learn generic player evolution
patterns from a historical cohort that includes both retired and
still-active players, and then apply these patterns to predict the
next-season performance of currently active players. We subsequently
aggregate predicted player statistics to the team level and compare the
resulting win forecasts to those from a simple standings-based baseline
over roughly two decades of NBA data. This setup allows us to evaluate
how a player-based forecasting pipeline and a standings-based model
perform side by side, especially in seasons where teams undergo
substantial roster changes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tasks and evaluation}
\label{sec:notation}

We study two related prediction tasks: (i) forecasting regular-season
team rankings from team-level summary statistics, and (ii)
predicting individual player statistics. Because the main results of
this paper concern team-level accuracy, we focus on the first task
here; the player-level task follows the same general protocol but
uses different feature inputs.

\subsection{Team-level ranking prediction}

For each team $t$ and season $s$, we assemble a feature vector
$\mathbf{x}_{t,s}$ from season-level box-score summaries and simple
trend indicators. The target is the realized winning percentage
\[
\text{WIN\_PCT}_{t,s}
=
\frac{\text{WINS}_{t,s}}
     {\text{WINS}_{t,s} + \text{LOSSES}_{t,s}}.
\]
A regression model $f_\theta$ produces a prediction
$\hat{y}_{t,s} = f_\theta(\mathbf{x}_{t,s})$, and teams are ranked within
each season by sorting $\hat{y}_{t,s}$ in descending order.

\paragraph{Data split.}
We use team–season summaries from the 2004–2005 through
2024–2025 seasons (630 observations in total). The two most
recent seasons (2023–2024 and 2024–2025) serve as a held-out
test set, and all earlier seasons form the training set. All feature
engineering is performed on the full panel, but model fitting uses
only the training seasons; performance is reported exclusively on
the held-out seasons.

\paragraph{Evaluation metrics.}
Because the task is ranking rather than regression, we evaluate
predictions using three rank-based metrics computed separately
for each season:
\[
\mathrm{ExactAcc}_s =
\frac{1}{N_s}\sum_t
\mathbb{1}[\mathrm{RANK}^{\mathrm{pred}}_{t,s}
=
\mathrm{RANK}^{\mathrm{true}}_{t,s}],
\]
\[
\mathrm{Within1}_s =
\frac{1}{N_s}\sum_t
\mathbb{1}[|\mathrm{RANK}^{\mathrm{pred}}_{t,s}
-
\mathrm{RANK}^{\mathrm{true}}_{t,s}|\le 1],
\]
\[
\mathrm{Within2}_s =
\frac{1}{N_s}\sum_t
\mathbb{1}[|\mathrm{RANK}^{\mathrm{pred}}_{t,s}
-
\mathrm{RANK}^{\mathrm{true}}_{t,s}|\le 2].
\]
We report each metric for both held-out seasons and also average them
for model comparison. Additional details on high-performing model
configurations are provided in Appendix~\ref{app:model-search}.
Figure~\ref{fig:rank-scatter} visualizes predicted versus actual rankings
for the two held-out test seasons.

% ===================== 两个赛季的排名散点图 =====================
\begin{figure*}
\centering
\begin{subfigure}{0.48\textwidth}
    \centering
    % 保存的文件名示例：figs/season_2023_2024_rank_scatter.png
    \includegraphics[width=\textwidth]{season_2023_2024_rank_scatter}
    \caption{2023--2024 season.}
    \label{fig:rank-2324}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \centering
    % 保存的文件名示例：figs/season_2024_2025_rank_scatter.png
    \includegraphics[width=\textwidth]{season_2024_2025_rank_scatter}
    \caption{2024--2025 season.}
    \label{fig:rank-2425}
\end{subfigure}
\caption{
Predicted versus actual team rankings for the two held-out seasons.
Each point corresponds to a single team; the dashed diagonal indicates
perfect agreement between predicted and true rankings.
}
\label{fig:rank-scatter}
\end{figure*}
% ==========================================================

\subsubsection{Feature groups}

To structure the search space, we organize the candidate
predictors into six groups and randomly sample combinations of
these groups during model search:
\begin{itemize}
  \item \textbf{basic\_box}: core box-score totals (PTS, FGA, FTA, TRB, AST, STL, BLK, TOV, PF),
  \item \textbf{basic\_eff}: efficiency ratios (TS\_PCT, EFG\_PCT, AST\_TO\_RATIO),
  \item \textbf{four\_factors}: EFG, turnover, offensive rebounding, and free-throw rate,
  \item \textbf{rolling\_box}: 3-year rolling averages of key box-score totals,
  \item \textbf{full\_box}: the full traditional box-score set,
  \item \textbf{trend}: previous-season win percentage, multi-year rolling averages, and a simple trend term.
\end{itemize}

\subsubsection{Models and hyperparameter search}

We perform a random search over model families, feature-group
combinations, and preprocessing strategies. The model families
include gradient boosting, random forests, LightGBM, XGBoost,
ridge and lasso regression, support vector regression, $k$-nearest
neighbors, and a small feed-forward MLP. Tree-based ensembles
consistently provide the strongest performance.

Each configuration is trained on the 2004–2005 through
2022–2023 seasons and evaluated on the two held-out seasons.
We retain all configurations achieving at least $40\%$ exact rank
accuracy for further examination.


The final XGBoost model uses a concrete set of 13 season-level variables,
listed in Table~\ref{tab:best-features}. These include nine
box-score totals (PTS, FGA, FTA, TRB, AST, STL, BLK, TOV, PF) and
four trend-based indicators (previous-season win percentage,
three- and five-year rolling averages, and a simple trend term).
This exact feature set corresponds to the model achieving
$63.3\%$ exact rank accuracy, $78.3\%$ within--1, and
$91.7\%$ within--2 accuracy across the two held-out seasons.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project description and design process (Unfinished)}
\label{sec:project}

This section describes our data, modeling choices, and the iterative
design process that led to our final pipeline.

\subsection{Data sources and preprocessing}

We rely on publicly available NBA box-score and standings data covering
approximately the past twenty regular seasons. For each season, we
extract:

\begin{itemize}
    \item \textbf{Team-level data:} wins, losses, point differential,
          offensive and defensive rating where available, and conference.
    \item \textbf{Player-level data:} per-game box-score statistics
          (points, rebounds, assists, steals, blocks, turnovers, personal
          fouls, three-pointers made and attempted, free-throws made and
          attempted), minutes played, and age.
    \item \textbf{Player-team mapping:} mapping from
          players to teams by season, and indicators for major roster
          changes (e.g., a team acquiring or losing a high-usage player).
\end{itemize}

We standardize statistics to a per-match basis where appropriate and handle missing values via simple imputation rules
(e.g., treating missing three-point attempts as zero for eras before the
three-point line, or dropping seasons with incomplete data). Players
below a minutes-played threshold are treated as replacement-level and
shrunk toward positional means.

\subsection{Model variants}

We consider several variants of the two core models.

\paragraph{Standings baseline variants.}
We evaluate:
\begin{enumerate}
    \item A  ``past years wins'' model using Eq.~\eqref{eq:standings-1y}.
    \item A multi-year regression using Eq.~\eqref{eq:player-evol}.
\end{enumerate}
These models are intentionally simple and interpretable, mirroring the
kind of baselines that fans and analysts often use in informal
discussions.

\paragraph{Player-based pipeline variants.}
On the player side, we explore:
\begin{enumerate}
    \item A linear evolution model as in Eq.~\eqref{eq:player-evol}.
    \item A regularized variant with position-specific parameters,
          allowing guards, wings, and bigs to follow different
          developmental trajectories.
    \item A baseline that skips the evolution step and directly uses
          last-season player statistics in Eq.~\eqref{eq:agg-player},
          effectively assuming no change in player performance.
\end{enumerate}
At the team level, we compare a model that uses only
$\widehat{\mathbf{x}}^{\text{player}}_{i,t+1}$ to one that augments
these features with the previous season's team wins $y_{i,t}$.

\subsection{Design process}

We approached the project as an iterative design problem with three
main phases.

\paragraph{Phase 1: Model selection.}
We began by prototyping several candidate baselines, including
Pythagorean expectation, Elo-style ratings, and the simple standings
regressions in Eqs.~\eqref{eq:standings-1y}--\eqref{eq:player-evol}.
Preliminary experiments and informal feedback from classmates suggested
that the ``last year wins'' baseline maintains a good balance between
simplicity and surprisingly strong performance. This motivated us to use
it as a primary comparison point.

\paragraph{Phase 2: Player-based model.}
We next experimented with alternative ways to link player trajectories
to team outcomes. Early versions attempted to directly map concatenated
player stats to wins, but these models were difficult to interpret and
prone to overfitting. The two-stage pipeline in
Eqs.~\eqref{eq:player-evol}--\eqref{eq:team-from-player} emerged as a
compromise between flexibility and interpretability.

\paragraph{Phase 3: Feature selection and regularization.}
Then, we iterated on feature sets and regularization strength using
held-out seasons as a validation set. 

\paragraph{Phase 4: Player to Team aggregate analysis.}
Finally, we mapped from player data to team to see if the prediction
accuracy remains high.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation, user testing, and iteration}
\label{sec:testing}

Although our project does not have ``end users'' in the sense of a
deployed app, we still treat model evaluation as a form of
user-centered testing. Our intended users are basketball fans and
analysts who would like forecasts that are both accurate and
interpretable.

\subsection{Train--test protocol}

To mimic a realistic forecasting setting, we adopt a rolling-origin
evaluation scheme. For each target season $t+1$ in a held-out range, we
train models using only data from seasons up to $t$ and then generate
out-of-sample predictions for $y_{i,t+1}$ for all teams.

\subsection{Metrics}

We use three main metrics to assess model quality and to guide
iterations on the design:

\begin{itemize}
    \item \textbf{Mean absolute error (MAE)} between predicted and
          actual win totals:
          \[
              \text{MAE}
              = \frac{1}{K}
                \sum_{(i,t) \in \mathcal{H}}
                \left| \widehat{y}_{i,t} - y_{i,t} \right| ,
          \]
    \item \textbf{Root mean squared error (RMSE)}, which penalizes large
          mistakes more heavily and highlights seasons where a model
          badly misjudges a team.
    \item \textbf{Rank correlation} (Spearman's $\rho$) between predicted
          and actual standings within each conference, which captures how
          well a model preserves the ordering of teams even if it is off
          by a few wins.
\end{itemize}

\subsection{Iterations}

We use these metrics to drive three kinds of iteration:

\paragraph{Feature-level iteration.}
When MAE and RMSE are high for particular kinds of teams, such as those
with many young players, we revisit the player features to ensure that
age and usage are represented explicitly. This helps the evolution model
better capture typical improvement or decline patterns.

\paragraph{Model-level iteration.}
We compare the baseline and player-based pipelines season by season. In
years where the player model underperforms, we inspect failure cases to
understand whether they arise from injuries, unusual rotations, or
outlier shooting performance. Based on these analyses, we adjust
regularization or revert to simpler models for especially noisy
features.

\paragraph{Interpretability checks with target users.}
Finally, we conducted informal ``user tests'' with classmates who
identify as NBA fans. We showed them
side-by-side visualizations of forecasts from the two models and asked
which explanations they found more intuitive. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Findings}
\label{sec:findings}

We summarize the main empirical findings from our comparison of the
standings baseline and the player-based model.

\subsection{Overall predictive performance}

Across seasons, the player-based model achieves lower MAE
and RMSE than the baseline model on average. All models achieved over $60\%$ accuracy.

\subsection{Interpretability and explanatory power}

From a qualitative perspective, the player-based model gives richer
explanations. When the model projects a team to improve, we can trace
that improvement back to specific players. Likewise, when it expects regression, we can point
to declining aging curves or the loss of particular player's contribution.

In contrast, the standings baseline model can only say that a team is expected
to win roughly as many games as last year.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}

Our findings highlight both the strength and the limitations of simple
team-level model.

On the one hand, historical standings are surprisingly informative. A
large fraction of the variance in next-season wins can be explained by
past-years wins, reflecting the reality that many teams change
relatively slowly. For fans or analysts who need a quick back-of-the-envelope
forecast, such model is a good way to know the general trend of a team.

On the other hand, the player-based model gives two key benefits:

\begin{enumerate}
    \item \textbf{Better handling of structural change.} When a team's players
          change substantially, the past win total of the \emph{team
          name} is less informative than the projected contributions of
          the new team. Our player-based model captures this by
          explicitly modeling individual contribution and aggregating
          them into team-level expectations.
    \item \textbf{Better narratives and interpretability.} Modern sports media and fan
          communities are not satisfied with current rating and predictions; they want
          to understand how and why a team is expected to rise or fall.
          By producing forecasts in predicted box-score contributions,
          our model supports narratives such as ``this team is
          expected to improve because its purchase of a super star are expected to
          become more efficient scorers.''
\end{enumerate}

From a design perspective, the project also illustrates how analytics
systems can balance simplicity and transparency. A purely black-box
model might achieve slightly better raw accuracy but would be difficult
to explain to non-technical users. Conversely, a naive baseline is easy
to understand but may fail precisely in the situations that most excite
fans. Our two-model comparison
helps clarify when additional complexity buys value and when it does
not.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and future work}
\label{sec:limitations}

Our study has several limitations that should be kept in mind when
interpreting the results.

\subsection{Data and modeling assumptions}

First, we rely primarily on traditional box-score data and simple
roster metadata. We do not incorporate richer tracking information and play-level detailed statistics. As a result, our player evolution model may miss
important aspects of defensive impact, spacing, and off-ball value.

Second, our evolution model assumes relatively smooth aging curves and
uses linear dynamics. In reality, player trajectories can be
nonlinear and injury-prone, and they can be strongly influenced by
changes in role, coaching, or offensive scheme. Our simple model
cannot capture all of these information.

\subsection{Evaluation constraints}

On the evaluation side, we are limited by the relatively small number
of independent seasons. Even when we adopt cross validation, there are only so
many non-overlapping train--test splits available. This makes it
difficult to obtain very tight estimates of performance differences
between models.

In addition, our informal ``user tests'' with classmates are not a
formal user study in the HCI sense. They involve a small, convenience
sample of basketball fans, and feedback is skewed rather than
systematically quantified.

\subsection{Directions for future work}

These limitations suggest several directions for future work:

\begin{itemize}
    \item Incorporating richer, opponent-side player impact metrics into the evolution
          model.
    \item Exploring nonlinear or hierarchical models for player
          trajectories, such as Gaussian processes or neural networks,
          while maintaining interpretability through careful feature
          design.
    \item Conducting a more systematic user study with sports analysts
          and fans to evaluate how different model explanations affect
          trust nd willingness to use forecasts in
          decision-making contexts.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusion}

We set out to answer a simple question: when is a model ``good enough'' for predicting NBA team performance, and when
is it worth investing in a more complex, player-centric model?

Our analysis suggests three main takeaways. First, historical standings
provide a strong and surprisingly robust prediction, especially for teams
with stable rosters. Second, when rosters change significantly, a
player-based pipeline that models individual evolution and aggregates
projected contributions can provide meaningful improvements in accuracy
and much richer explanations for the team's performance. Third, the choice between models should
depend on both the structure of the forecasting problem and the needs
of the intended users: those who value transparency and narrative may
prefer player-based model.

More broadly, our project illustrates how sports analytics tools can be
designed to balance simplicity, interpretability, and responsiveness to
change. By explicitly comparing team-level and player-level views of
the same forecasting task, we highlight the trade-offs that practitioners
confront when building models for real-world sports decision-making.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acknowledgements}
We thank the open basketball-reference and NBA Stats communities for
making historical data accessible, and the Northwestern MSCS program
for general support of this work.
\end{acknowledgements}

% ==== 参考文献：保持 A&A 的 aa.bst，author–year 风格 ====
\bibliographystyle{aa}
\bibliography{refs}

\begin{appendix}
\section{Additional team-level model search results}
\label{app:model-search}

\begin{table*}
\centering
\caption{Representative high-performing configurations from the random search over team-level ranking models. All listed runs achieve test-time exact rank accuracy $\mathrm{ExactAcc} \ge 0.40$ on average across the 2023--2024 and 2024--2025 seasons.}
\label{tab:model-search}
\scriptsize
\begin{tabular}{llcc}
\hline\hline
Feature groups & Model & ExactAcc & Within1 \\
\hline
basic\_box + trend
  & XGBoost (RobustScaler)
  & 0.633 & 0.783 \\

basic\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.617 & 0.767 \\

basic\_box + trend
  & LightGBM (no scaling)
  & 0.600 & 0.767 \\

basic\_box + trend
  & Random Forest (no scaling)
  & 0.583 & 0.750 \\

basic\_box + trend
  & MLP regressor (StandardScaler)
  & 0.450 & 0.700 \\

basic\_box + full\_box + trend
  & XGBoost (RobustScaler)
  & 0.617 & 0.783 \\

basic\_box + full\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.600 & 0.767 \\

basic\_box + full\_box + trend
  & LightGBM (StandardScaler)
  & 0.583 & 0.750 \\

basic\_box + four\_factors + trend
  & XGBoost (RobustScaler)
  & 0.600 & 0.783 \\

basic\_box + four\_factors + trend
  & Gradient Boosting (RobustScaler)
  & 0.583 & 0.767 \\

basic\_box + four\_factors + trend
  & Random Forest (no scaling)
  & 0.567 & 0.750 \\

basic\_box + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.600 & 0.767 \\

basic\_box + rolling\_box + trend
  & LightGBM (RobustScaler)
  & 0.583 & 0.767 \\

basic\_box + rolling\_box + trend
  & Random Forest (no scaling)
  & 0.550 & 0.733 \\

four\_factors + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.567 & 0.750 \\

four\_factors + rolling\_box + trend
  & Gradient Boosting (StandardScaler)
  & 0.550 & 0.733 \\

four\_factors + rolling\_box + trend
  & LightGBM (no scaling)
  & 0.533 & 0.733 \\

basic\_eff + trend
  & XGBoost (StandardScaler)
  & 0.517 & 0.717 \\

basic\_eff + trend
  & Gradient Boosting (StandardScaler)
  & 0.500 & 0.700 \\

basic\_eff + trend
  & Random Forest (no scaling)
  & 0.483 & 0.700 \\

basic\_eff + four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.517 & 0.717 \\

basic\_eff + four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.500 & 0.700 \\

basic\_eff + four\_factors + trend
  & LightGBM (StandardScaler)
  & 0.483 & 0.700 \\

full\_box + trend
  & XGBoost (RobustScaler)
  & 0.583 & 0.767 \\

full\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.567 & 0.750 \\

full\_box + trend
  & LightGBM (StandardScaler)
  & 0.550 & 0.750 \\

full\_box + rolling\_box + trend
  & XGBoost (RobustScaler)
  & 0.583 & 0.767 \\

full\_box + rolling\_box + trend
  & Gradient Boosting (RobustScaler)
  & 0.567 & 0.750 \\

full\_box + rolling\_box + trend
  & Random Forest (no scaling)
  & 0.550 & 0.733 \\

basic\_box + basic\_eff + trend
  & XGBoost (StandardScaler)
  & 0.550 & 0.750 \\

basic\_box + basic\_eff + trend
  & Gradient Boosting (StandardScaler)
  & 0.533 & 0.733 \\

basic\_box + basic\_eff + trend
  & LightGBM (StandardScaler)
  & 0.517 & 0.733 \\

basic\_box + basic\_eff + four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.550 & 0.750 \\

basic\_box + basic\_eff + four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.533 & 0.733 \\

basic\_box + basic\_eff + four\_factors + trend
  & Random Forest (no scaling)
  & 0.517 & 0.717 \\

four\_factors + trend
  & XGBoost (StandardScaler)
  & 0.500 & 0.717 \\

four\_factors + trend
  & Gradient Boosting (StandardScaler)
  & 0.483 & 0.700 \\

four\_factors + trend
  & Random Forest (no scaling)
  & 0.467 & 0.700 \\

trend only
  & Ridge regression (StandardScaler)
  & 0.433 & 0.667 \\

trend only
  & Lasso regression (StandardScaler)
  & 0.417 & 0.650 \\

trend only
  & SVR (StandardScaler)
  & 0.417 & 0.650 \\

trend only
  & KNN regressor (MinMaxScaler)
  & 0.400 & 0.633 \\
\hline
\end{tabular}
\end{table*}

\end{appendix}

\end{document}
